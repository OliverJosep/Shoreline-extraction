{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the src directory to the path. TEMPORARY FIX\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../..\")))\n",
    "\n",
    "from models.attention_unet.attention_unet import Attention_UNet\n",
    "from models.attention_unet.attention_formes import Attention_UNet_Formes\n",
    "from src.data_processing.dataset_loader import CoastData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoastData: arenaldentem - 40 images\n",
      "Coast: arenaldentem, Total size: 40\n"
     ]
    }
   ],
   "source": [
    "# Load the data to split it and save it to a dict\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/processed/\"))\n",
    "data = CoastData(data_path=path, name=\"arenaldentem\")\n",
    "\n",
    "data_split = data.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Attention_UNet(num_classes=3, experiment_name=\"test_experiments\", use_mlflow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to the model\n",
    "data = unet.load_data(data_split, Attention_UNet_Formes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folders for the artifacts at /Users/josep/Documents/SOCIB/Shoreline-extraction/artifacts/2025-02-25-13-17-10\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/visio/lib/python3.9/site-packages/torcheval/metrics/functional/classification/accuracy.py:275: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:234.)\n",
      "  num_correct = mask.new_zeros(num_classes).scatter_(0, target, mask, reduce=\"add\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: \n",
      "\ttrain_loss: 1.024581789970398\n",
      "\ttrain_accuracy: 0.5057579874992371\n",
      "\ttrain_f1_score: 0.42513158917427063\n",
      "\ttrain_precision: 0.47883322834968567\n",
      "\ttrain_recall: 0.5057579874992371\n",
      "\ttrain_confusion_matrix: tensor([[14940.,  1518.,  1889.],\n",
      "        [28692., 34020.,  8107.],\n",
      "        [13519.,  6322.,  5681.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 20661.728515625\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss improved from inf to 20661.728516. Saving the model. Early stopping counter: 0/25\n",
      "Epoch 2/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.632439136505127\n",
      "\ttrain_accuracy: 0.8146354556083679\n",
      "\ttrain_f1_score: 0.778717041015625\n",
      "\ttrain_precision: 0.7582213282585144\n",
      "\ttrain_recall: 0.8146354556083679\n",
      "\ttrain_confusion_matrix: tensor([[15387.,  1280.,  1680.],\n",
      "        [ 9331., 56609.,  4879.],\n",
      "        [  684.,  4270., 20568.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 2655281.5\n",
      "\tvalidation_accuracy: 0.33380988240242004\n",
      "\tvalidation_f1_score: 0.10754867643117905\n",
      "\tvalidation_precision: 0.2289336919784546\n",
      "\tvalidation_recall: 0.33380988240242004\n",
      "\tvalidation_confusion_matrix: tensor([[ 5268.,   105.,     0.],\n",
      "        [20027.,   429.,     0.],\n",
      "        [ 6651.,   288.,     0.]])\n",
      "\n",
      "Validation loss did not improve from 20661.728515625, actual loss 2655281.500000. Early stopping counter: 0/25\n",
      "Epoch 3/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.43078087270259857\n",
      "\ttrain_accuracy: 0.8874886631965637\n",
      "\ttrain_f1_score: 0.8850641250610352\n",
      "\ttrain_precision: 0.8834540843963623\n",
      "\ttrain_recall: 0.8874886631965637\n",
      "\ttrain_confusion_matrix: tensor([[15940.,  1738.,   669.],\n",
      "        [ 1002., 65413.,  4404.],\n",
      "        [  827.,  2491., 22204.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 249040.3125\n",
      "\tvalidation_accuracy: 0.3341480791568756\n",
      "\tvalidation_f1_score: 0.09566349536180496\n",
      "\tvalidation_precision: 0.3880738317966461\n",
      "\tvalidation_recall: 0.3341480791568756\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20406.,    50.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss did not improve from 20661.728515625, actual loss 249040.312500. Early stopping counter: 1/25\n",
      "Epoch 4/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.3291379511356354\n",
      "\ttrain_accuracy: 0.9156231880187988\n",
      "\ttrain_f1_score: 0.9084339737892151\n",
      "\ttrain_precision: 0.9047159552574158\n",
      "\ttrain_recall: 0.9156231880187988\n",
      "\ttrain_confusion_matrix: tensor([[16377.,  1088.,   882.],\n",
      "        [  679., 65447.,  4693.],\n",
      "        [  336.,  1448., 23738.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 48287.5546875\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss did not improve from 20661.728515625, actual loss 48287.554688. Early stopping counter: 2/25\n",
      "Epoch 5/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.2542325258255005\n",
      "\ttrain_accuracy: 0.9241016507148743\n",
      "\ttrain_f1_score: 0.9165744185447693\n",
      "\ttrain_precision: 0.9138620495796204\n",
      "\ttrain_recall: 0.9241016507148743\n",
      "\ttrain_confusion_matrix: tensor([[16415.,   813.,  1119.],\n",
      "        [  528., 65717.,  4574.],\n",
      "        [  144.,  1141., 24237.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 9579.3720703125\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss improved from 20661.728516 to 9579.372070. Saving the model. Early stopping counter: 0/25\n",
      "Epoch 6/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.24933741241693497\n",
      "\ttrain_accuracy: 0.9030271172523499\n",
      "\ttrain_f1_score: 0.9064307808876038\n",
      "\ttrain_precision: 0.9118226170539856\n",
      "\ttrain_recall: 0.9030271172523499\n",
      "\ttrain_confusion_matrix: tensor([[1.6149e+04, 9.3200e+02, 1.2660e+03],\n",
      "        [5.7200e+02, 6.6731e+04, 3.5160e+03],\n",
      "        [3.7000e+01, 2.8570e+03, 2.2628e+04]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 3320.97216796875\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss improved from 9579.372070 to 3320.972168. Saving the model. Early stopping counter: 0/25\n",
      "Epoch 7/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.18579000979661942\n",
      "\ttrain_accuracy: 0.9344270825386047\n",
      "\ttrain_f1_score: 0.9344784617424011\n",
      "\ttrain_precision: 0.9352774620056152\n",
      "\ttrain_recall: 0.9344270825386047\n",
      "\ttrain_confusion_matrix: tensor([[16681.,   886.,   780.],\n",
      "        [  418., 68091.,  2310.],\n",
      "        [  428.,  1292., 23802.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 1325.068359375\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss improved from 3320.972168 to 1325.068359. Saving the model. Early stopping counter: 0/25\n",
      "Epoch 8/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.15399623662233353\n",
      "\ttrain_accuracy: 0.9386605620384216\n",
      "\ttrain_f1_score: 0.941227376461029\n",
      "\ttrain_precision: 0.9439473748207092\n",
      "\ttrain_recall: 0.9386605620384216\n",
      "\ttrain_confusion_matrix: tensor([[17003.,   879.,   465.],\n",
      "        [  535., 68530.,  1754.],\n",
      "        [  271.,  1731., 23520.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 540.6497192382812\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss improved from 1325.068359 to 540.649719. Saving the model. Early stopping counter: 0/25\n",
      "Epoch 9/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.11614328995347023\n",
      "\ttrain_accuracy: 0.9573163986206055\n",
      "\ttrain_f1_score: 0.9583781361579895\n",
      "\ttrain_precision: 0.959635317325592\n",
      "\ttrain_recall: 0.9573163986206055\n",
      "\ttrain_confusion_matrix: tensor([[17181.,   742.,   424.],\n",
      "        [  498., 69151.,  1170.],\n",
      "        [  162.,   883., 24477.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 231.71939086914062\n",
      "\tvalidation_accuracy: 0.3333333432674408\n",
      "\tvalidation_f1_score: 0.0939146876335144\n",
      "\tvalidation_precision: 0.054656982421875\n",
      "\tvalidation_recall: 0.3333333432674408\n",
      "\tvalidation_confusion_matrix: tensor([[ 5373.,     0.,     0.],\n",
      "        [20456.,     0.,     0.],\n",
      "        [ 6939.,     0.,     0.]])\n",
      "\n",
      "Validation loss improved from 540.649719 to 231.719391. Saving the model. Early stopping counter: 0/25\n",
      "Epoch 10/10\n",
      "train metrics: \n",
      "\ttrain_loss: 0.12422206997871399\n",
      "\ttrain_accuracy: 0.9600777626037598\n",
      "\ttrain_f1_score: 0.9507853984832764\n",
      "\ttrain_precision: 0.9429610371589661\n",
      "\ttrain_recall: 0.9600777626037598\n",
      "\ttrain_confusion_matrix: tensor([[17633.,   453.,   261.],\n",
      "        [  374., 67438.,  3007.],\n",
      "        [  373.,   472., 24677.]])\n",
      "\n",
      "validation metrics: \n",
      "\tvalidation_loss: 129.59547424316406\n",
      "\tvalidation_accuracy: 0.3324833810329437\n",
      "\tvalidation_f1_score: 0.09386774152517319\n",
      "\tvalidation_precision: 0.4196203649044037\n",
      "\tvalidation_recall: 0.3324833810329437\n",
      "\tvalidation_confusion_matrix: tensor([[5.3580e+03, 1.5000e+01, 0.0000e+00],\n",
      "        [2.0454e+04, 2.0000e+00, 0.0000e+00],\n",
      "        [6.9340e+03, 4.0000e+00, 1.0000e+00]])\n",
      "\n",
      "Validation loss improved from 231.719391 to 129.595474. Saving the model. Early stopping counter: 0/25\n"
     ]
    }
   ],
   "source": [
    "artifact_path = os.path.abspath(os.path.join(path, \"../../artifacts/\"))\n",
    "\n",
    "description = \"\"\n",
    "\n",
    "# Train the model\n",
    "unet.train(epochs=10, artifact_path=artifact_path, run_description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
